## # Caching
call`.cache ()` on the DataFrame before Action
```py
voter_df spark.read.csv("voter_data.txt.gz')
voter_df.cache ().count ()

voter_df = voter_df. withColumn ("IDÂ°, monotonically_increasing_id ())
voter_df = voter_df.cache (O
voter_df.show()
```
### # more chache operators
Check `.is_cached` to determine cache status
```py
print (voter_df.is_cached)
```
    True
Call `unpersist ()` when finished with DataFrame
## Caching a DataFrame
- [x] Cache the unique rows in the departures_df DataFrame.
- [x] Perform a count query on departures_df, noting how long the operation takes.
- [x] Count the rows again, noting the variance in time of a cached DataFrame.
```py
start_time = time.time()

# Add caching to the unique rows in departures_df
departures_df = departures_df.distinct().cache()

# Count the unique rows in departures_df, noting how long the operation takes
print("Counting %d rows took %f seconds" %
      (departures_df.count(), time.time() - start_time))

# Count the rows again, noting the variance in time of a cached DataFrame
start_time = time.time()
print("Counting %d rows again took %f seconds" %
      (departures_df.count(), time.time() - start_time))
```
## Removing a DataFrame from cache
- [x] Check the caching status on the departures_df DataFrame.
- [x] Remove the departures_df DataFrame from the cache.
- [x] Validate the caching status again.
```py
# Determine if departures_df is in the cache
print("Is departures_df cached?: %s" % departures_df.is_cached)
print("Removing departures_df from cache")

# Remove departures_df from the cache
departures_df.unpersist()

# Check the cache status again
print("Is departures_df cached?: %s" % departures_df.is_cached)

'''
Is departures_df cached?: True
Removing departures_df from cache
Is departures_df cached?: False
'''
```
## # Improve import performance
`import performance`
- Can import via wildcard
```py
airport.df = spark.read.CSV('airports-*.txt.gz')
```
### # How to split objects
- Use OS utilities / scripts `(split, cut, awk)`
```py
split -l 10000 -d largefile chunk-
```
- Use custom scripts
- Write out to Parquet
```py
df_csv = spark.read.csv('singlelargefile.csv')
df_csv.Write.parquet("data.parquet")
df = spark.read.parquet("data.parquet")
```
## File size optimization
Consider if you're given 2 large data files on a cluster with 10 nodes. Each file contains 10M rows of roughly the same size. While working with your data, the responsiveness is acceptable but the initial read from the files takes a considerable period of time. Note that you are the only one who will use the data and it changes for each run.

> Which of the following is the best option to improve performance?

Answer the question
- [ ] Split the 2 files into 8 files of 2.5M rows each.
- [ ] Convert the files to parquet.
- [x] Split the 2 files into 50 files of 400K rows each.
- [ ] Split the files into 30 files containing a random number of rows.
## File import performance
- [x] Import the departures_full.txt.gz file and the departures_xxx.txt.gz files into separate DataFrames.
- [x] Run a count on each DataFrame and compare the run times.
```py
# Import the full and split files into DataFrames
full_df = spark.read.csv('departures_full.txt.gz')
split_df = spark.read.csv('departures_*.txt.gz')

# Print the count and run time for each DataFrame
start_time_a = time.time()
print("Total rows in full DataFrame:\t%d" % full_df.count())
print("Time to run: %f" % (time.time() - start_time_a))

start_time_b = time.time()
print("Total rows in split DataFrame:\t%d" % split_df.count())
print("Time to run: %f" % (time.time() - start_time_b))

'''
Total rows in full DataFrame:	139359
Time to run: 0.314305
Total rows in split DataFrame:	278718
Time to run: 0.569276
'''
```
